{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25aaf1b5",
   "metadata": {},
   "source": [
    "# <center> **Titanic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c8da2",
   "metadata": {},
   "source": [
    "# **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73e35264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions' from 'c:\\\\Users\\\\Dell\\\\Documents\\\\AI\\\\Titanic\\\\Notebooks\\\\functions.py'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import functions\n",
    "import importlib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "importlib.reload(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169bb5d",
   "metadata": {},
   "source": [
    "# **Data Overview and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e358ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    r\"C:\\Users\\Dell\\Documents\\AI\\Titanic\\Data\\data.csv\",\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "train = pd.read_csv(\n",
    "    r\"C:\\Users\\Dell\\Documents\\AI\\Titanic\\Data\\Data\\train.csv\",\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "test = pd.read_csv(\n",
    "    r\"C:\\Users\\Dell\\Documents\\AI\\Titanic\\Data\\Data\\test.csv\",\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "\n",
    "random_state = 101\n",
    "target = 'Transported'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c97564",
   "metadata": {},
   "source": [
    "# **Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3663d",
   "metadata": {},
   "source": [
    "## **Split Data Back to Train and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a099298",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data[data['PassengerId'].isin(train['PassengerId'].values)].copy()\n",
    "test=data[data['PassengerId'].isin(test['PassengerId'].values)].copy()\n",
    "test.drop(columns=target, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c32b5d",
   "metadata": {},
   "source": [
    "## **Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4404734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(target, axis=1)\n",
    "y = train[target]\n",
    "\n",
    "X, y = shuffle(X, y, random_state=random_state)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d267b4e",
   "metadata": {},
   "source": [
    "# **Drop Unneeded Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8f18a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['PassengerId', 'Group', 'CabinNumber'], axis=1, inplace=True)\n",
    "test.drop(['PassengerId', 'Group', 'CabinNumber'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f93224",
   "metadata": {},
   "source": [
    "# **Log Transform**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b28817",
   "metadata": {},
   "source": [
    "The logarithm transform is used to decrease skew in distributions, especially with large outliers. It can make it easier for algorithms to 'learn' the correct relationships. We will apply it to the expenditure features as these are heavily skewed by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f966e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['RoomService'] = np.log1p(X['RoomService'])\n",
    "test['RoomService'] = np.log1p(test['RoomService'])\n",
    "\n",
    "X['FoodCourt'] = np.log1p(X['FoodCourt'])\n",
    "test['FoodCourt'] = np.log1p(test['FoodCourt'])\n",
    "\n",
    "X['ShoppingMall'] = np.log1p(X['ShoppingMall'])\n",
    "test['ShoppingMall'] = np.log1p(test['ShoppingMall'])\n",
    "\n",
    "X['Spa'] = np.log1p(X['Spa'])   \n",
    "test['Spa'] = np.log1p(test['Spa'])\n",
    "\n",
    "X['VRDeck'] = np.log1p(X['VRDeck'])\n",
    "test['VRDeck'] = np.log1p(test['VRDeck'])\n",
    "\n",
    "X['Spent'] = np.log1p(X['Spent'])\n",
    "test['Spent'] = np.log1p(test['Spent'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ad7f4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupSize         int64\n",
       "Solo              int64\n",
       "HomePlanet       object\n",
       "CabinDeck        object\n",
       "CabinSide        object\n",
       "CryoSleep          bool\n",
       "Destination      object\n",
       "Age             float64\n",
       "AgeGroup         object\n",
       "VIP                bool\n",
       "RoomService     float64\n",
       "FoodCourt       float64\n",
       "ShoppingMall    float64\n",
       "Spa             float64\n",
       "VRDeck          float64\n",
       "TotalSpent      float64\n",
       "Spent           float64\n",
       "LastName         object\n",
       "FamilySize        int64\n",
       "LoneTraveler      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c6a7b",
   "metadata": {},
   "source": [
    "# **Encoding and Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9c703d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'PassengerId', 'Transported', 'Group', 'CabinNumber'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[0;32m     23\u001b[0m train \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mfit_transform(train)\n\u001b[1;32m---> 24\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Print new shape\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining set shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\Documents\\AI\\Titanic\\titanicvenv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\Documents\\AI\\Titanic\\titanicvenv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1065\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1063\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m-> 1065\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: columns are missing: {'PassengerId', 'Transported', 'Group', 'CabinNumber'}"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Indentify numerical and categorical columns\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "categorical_cols = [cname for cname in X.columns if X[cname].dtype == [\"object\", 'bool']]\n",
    "\n",
    "# Scale numerical data to have mean=0 and variance=1\n",
    "numerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "# One-hot encode categorical data\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop='if_binary', handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "# Apply preprocessing\n",
    "train = ct.fit_transform(train)\n",
    "test = ct.transform(test)\n",
    "\n",
    "# Print new shape\n",
    "print('Training set shape:', train.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
