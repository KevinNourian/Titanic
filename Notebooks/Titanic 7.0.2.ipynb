{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import functions\n",
    "import importlib\n",
    "importlib.reload(functions)\n",
    "\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Overview and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    r\"C:\\Users\\Dell\\Documents\\AI\\Titanic\\Data\\data.csv\",\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "train = pd.read_csv(\n",
    "    r\"C:\\Users\\Dell\\Documents\\AI\\Titanic\\Data\\Data\\train.csv\",\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "test = pd.read_csv(\n",
    "    r\"C:\\Users\\Dell\\Documents\\AI\\Titanic\\Data\\Data\\test.csv\",\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "\n",
    "random_state = 101\n",
    "target = 'Transported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Split Data Back to Train and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data[data['PassengerId'].isin(train['PassengerId'].values)].copy()\n",
    "test=data[data['PassengerId'].isin(test['PassengerId'].values)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Drop Unneeded Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['PassengerId', 'Group', 'CabinNumber'], axis=1, inplace=True)\n",
    "test.drop(['PassengerId', 'Group', 'CabinNumber'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Log Transform**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logarithm transform is used to decrease skew in distributions, especially with large outliers. It can make it easier for algorithms to 'learn' the correct relationships. We will apply it to the expenditure features as these are heavily skewed by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_transform = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'TotalSpent']  # Replace with your actual column names\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    train = functions.log_transform(train, col)\n",
    "    test = functions.log_transform(test, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Column Separation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [cname for cname in train.columns if train[cname].dtype in ['int64', 'float64']]\n",
    "categorical_cols = [cname for cname in train.columns if train[cname].dtype in [\"object\", \"bool\"]]\n",
    "categorical_cols.remove(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(target, axis=1)\n",
    "y = train[target]\n",
    "y = y.astype(bool)\n",
    "\n",
    "\n",
    "X, y = shuffle(X, y, random_state=random_state)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression: Unlike linear regression which uses Least Squares, this model uses Maximum Likelihood Estimation to fit a sigmoid-curve on the target variable distribution. The sigmoid/logistic curve is commonly used when the data is questions had binary output.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN works by selecting the majority class of the k-nearest neighbours, where the metric used is usually Euclidean distance. It is a simple and effective algorithm but can be sensitive by many factors, e.g. the value of k, the preprocessing done to the data and the metric used.\n",
    "\n",
    "Random Forest (RF): RF is a reliable ensemble of decision trees, which can be used for regression or classification problems. Here, the individual trees are built via bagging (i.e. aggregation of bootstraps which are nothing but multiple train datasets created via sampling with replacement) and split using fewer features. The resulting diverse forest of uncorrelated trees exhibits reduced variance; therefore, is more robust towards change in data and carries its prediction accuracy to new data. It works well with both continuous & categorical data.\n",
    "\n",
    "Extreme Gradient Boosting (XGBoost): XGBoost is similar to RF in that it is made up of an ensemble of decision-trees. The difference arises in how those trees as derived; XGboost uses extreme gradient boosting when optimising its objective function. It often produces the best results but is relatively slow compared to other gradient boosting algorithms.\n",
    "\n",
    "Light Gradient Boosting Machine (LGBM): LGBM works essentially the same as XGBoost but with a lighter boosting technique. It usually produces similar results to XGBoost but is significantly faster.\n",
    "\n",
    "Categorical Boosting (CatBoost): CatBoost is an open source algorithm based on gradient boosted decision trees. It supports numerical, categorical and text features. It works well with heterogeneous data and even relatively small data. Informally, it tries to take the best of both worlds from XGBoost and LGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.77 (0.73 minutes)\n",
      "KNN: 0.75 (0.20 minutes)\n",
      "Random Forest: 0.79 (2.55 minutes)\n",
      "XGBoost: 0.80 (0.80 minutes)\n",
      "LightGBM: 0.80 (0.16 minutes)\n",
      "CatBoost: 0.81 (5.00 minutes)\n"
     ]
    }
   ],
   "source": [
    "numerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "preprocessor  = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "\n",
    "lg_model = LogisticRegression(random_state=random_state, max_iter=5000)\n",
    "lg_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lg', lg_model)\n",
    "])\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('knn', knn_model)\n",
    "])  \n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=random_state)\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('random_forest', rf_model)\n",
    "])\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', xgb_model)\n",
    "])\n",
    "\n",
    "lgbm_model = LGBMClassifier(random_state=random_state, verbose=0)\n",
    "lgbm_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lgbm', lgbm_model)\n",
    "])\n",
    "\n",
    "catboost_model = CatBoostClassifier(random_state=random_state, verbose=0)\n",
    "catboost_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('catboost', catboost_model)\n",
    "])  \n",
    "\n",
    "\n",
    "pipelines = {\n",
    "    \"Logistic Regression\": lg_pipeline,\n",
    "    \"KNN\": knn_pipeline,\n",
    "    \"Random Forest\": rf_pipeline,\n",
    "    \"XGBoost\": xgb_pipeline,\n",
    "    \"LightGBM\": lgbm_pipeline,\n",
    "    \"CatBoost\": catboost_pipeline,\n",
    "}\n",
    "\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    start_time = time.time()  \n",
    "    scores = cross_val_score(pipeline, X, y, cv=10)\n",
    "    end_time = time.time()  \n",
    "    elapsed_time = (end_time - start_time)/60  \n",
    "    \n",
    "    print(f\"{name}: {scores.mean():.2f} ({elapsed_time:.2f} minutes)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
